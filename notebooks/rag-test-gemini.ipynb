{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Evaluating the Retrieval Augmented Generation (RAG) Flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Building the RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Read Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import minsearch\n",
    "from tqdm.auto import tqdm\n",
    "import google.auth\n",
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set up the API key and project ID for Gemini \n",
    "PROJECT_ID = os.environ['GCP_PROJECT_ID']\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"../pacific-ethos-428312-n5-eb4864ff3add.json\"\n",
    ")\n",
    "vertexai.init(project=PROJECT_ID, credentials=credentials, location=\"us-central1\")\n",
    "\n",
    "# Path to your data JSONL file\n",
    "file_path = '../data/bq-results-20240829-041517-1724904953827.jsonl'\n",
    "\n",
    "# Read the JSONL file directly into a Pandas DataFrame\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Data with Minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['abstract', 'authors', 'keywords', 'organization_affiliated', 'title', 'id'],\n",
    "    keyword_fields=['id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x1fc70c61a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = df.to_dict(orient='records')\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests\n",
    "# query = 'Which articles discuss correlation between smoking and pregnancy?'\n",
    "# results = search(query)\n",
    "# query = \"Provide a summary of insights found in articles that discuss pregnant women exposured to smoking\"\n",
    "# results = search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're an experienced biomedical researcher. Answer the QUESTION based only on the CONTEXT from our Biomedical Research database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION. Your answer must be an accurate summary and not an exact copy of the text. \n",
    "However, article titles, authors, keywords, and organizations must be exact from the CONTEXT. \n",
    "Do NOT include any article that does NOT exist in the CONTEXT.\n",
    "Do NOT include anything that does NOT answer the QUESTION.\n",
    "Do NOT repeat ANYTHING that you have previously said in your response.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "entry_template = \"\"\"\n",
    "abstract: {abstract}\n",
    "authors: {authors} \n",
    "keywords: {keywords} \n",
    "organization_affiliated: {organization_affiliated} \n",
    "title: {title}\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "# print(build_prompt(query, results))\n",
    "# prompt = build_prompt(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model=\"gemini-1.5-flash-001\"):\n",
    "    model = GenerativeModel(model)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "# answer = llm(prompt)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model='gemini-1.5-flash-001'):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    #print(prompt)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article \"Interaction of glutathione S-transferase polymorphisms and tobacco smoking during pregnancy in susceptibility to autism spectrum disorders\" by Vanja Mandic-Maravic et al. discusses a possible correlation between maternal smoking during pregnancy and the risk of Autism Spectrum Disorders (ASD).  However, the study found that maternal smoking during pregnancy did not increase the risk of ASD. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = 'Provide a summary of insights found in articles that discuss correlation between pregnancy and smoking'\n",
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Join Data Source and Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file directly into a Pandas DataFrame\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c3ea29df-6683-4443-a2c7-3f027137c1d8</td>\n",
       "      <td>What are some systemic factors that influence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c3ea29df-6683-4443-a2c7-3f027137c1d8</td>\n",
       "      <td>What types of systemic factors should be consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3ea29df-6683-4443-a2c7-3f027137c1d8</td>\n",
       "      <td>What is the role of systemic factors in determ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c3ea29df-6683-4443-a2c7-3f027137c1d8</td>\n",
       "      <td>How do systemic factors impact treatment plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c3ea29df-6683-4443-a2c7-3f027137c1d8</td>\n",
       "      <td>In the context of periodontitis reassessment, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  c3ea29df-6683-4443-a2c7-3f027137c1d8   \n",
       "1  c3ea29df-6683-4443-a2c7-3f027137c1d8   \n",
       "2  c3ea29df-6683-4443-a2c7-3f027137c1d8   \n",
       "3  c3ea29df-6683-4443-a2c7-3f027137c1d8   \n",
       "4  c3ea29df-6683-4443-a2c7-3f027137c1d8   \n",
       "\n",
       "                                            question  \n",
       "0  What are some systemic factors that influence ...  \n",
       "1  What types of systemic factors should be consi...  \n",
       "2  What is the role of systemic factors in determ...  \n",
       "3  How do systemic factors impact treatment plann...  \n",
       "4  In the context of periodontitis reassessment, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the ground truth CSV file directly into a Pandas DataFrame\n",
    "ground_truth_df = pd.read_csv('../data/ground-truth-retrieval.csv')\n",
    "\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'c3ea29df-6683-4443-a2c7-3f027137c1d8',\n",
       " 'question': 'What are some systemic factors that influence treatment decisions for residual periodontal probing depths?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert ground truth data to JSON \n",
    "ground_truth = ground_truth_df.to_dict(orient='records')\n",
    "\n",
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data shape: (200, 6)\n",
      "Number of unique ids: 200\n"
     ]
    }
   ],
   "source": [
    "df_sample = df[df['id'].isin(list(ground_truth_df['id']))].reset_index(drop=True)\n",
    "print('Evaluation data shape:', df_sample.shape)\n",
    "print('Number of unique ids:', len(df_sample['id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Evaluation Data with Minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x21b4ef918e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['abstract', 'authors', 'keywords', 'organization_affiliated', 'title', 'id'],\n",
    "    keyword_fields=['id']\n",
    ")\n",
    "\n",
    "documents = df_sample.to_dict(orient='records')\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def minsearch_search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302af78ecca544c3bbc155088eeacf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.959, 'mrr': 0.8424214285714289}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: minsearch_search(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')  # Assuming we're minimizing. Use float('-inf') if maximizing.\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Generate random parameters\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                current_params[param] = random.randint(min_val, max_val)\n",
    "            else:\n",
    "                current_params[param] = random.uniform(min_val, max_val)\n",
    "        \n",
    "        # Evaluate the objective function\n",
    "        current_score = objective_function(current_params)\n",
    "        \n",
    "        # Update best if current is better\n",
    "        if current_score > best_score:  # Change to > if maximizing\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "def minsearch_search(query, boost=None):\n",
    "    if boost is None:\n",
    "        boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return minsearch_search(q['question'], boost_params)\n",
    "\n",
    "    results = evaluate(ground_truth, search_function)\n",
    "    return results['mrr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c162037cc6241ddb1651ae8a5125a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331e02beb084491198276da025a0f204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951a6548f39a4cde90f24f074f57caaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1882c19550dd4403a1639d0113ea3ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f7524435d449c8a70216ccc1878374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca13b774c7eb44bc8904a2b2213fc736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2abfc91b1a747be80e8e3b6dc8a29b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c25e7735194c5b8279fa0bbf547ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fc0bb4c4e0439ca0fd520ae29d7cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c974e9181def4a63800da46fe55c889d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f790476d54441e8eeb34995ac44ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c68568d218438da0d010595da62658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d2f4fddf094dd0aebffcd30cf193a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aae49783214db1b333312188779708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6b339d063e4b9ba03ce81d841f9fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa92f0b79d834eeb9d4b906b79319e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1c7a5434da49f5a1c9cdf907e79660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e29879b3c5247bc91acd929949a9c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1374cb95089549169997f690e9cc994b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ab5524af3b44fd8484c47c0a63fa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({'abstract': 2.3804731710209435,\n",
       "  'authors': 0.033858106315256986,\n",
       "  'keywords': 0.5177450395210756,\n",
       "  'organization_affiliated': 1.3341455155330277,\n",
       "  'title': 0.1961060756966928},\n",
       " 0.9655884920634924)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_ranges = {\n",
    "    'abstract': (0.0, 3.0),\n",
    "    'authors': (0.0, 3.0),\n",
    "    'keywords': (0.0, 3.0),\n",
    "    'organization_affiliated': (0.0, 3.0),\n",
    "    'title': (0.0, 3.0)\n",
    "}\n",
    "simple_optimize(param_ranges, objective, n_iterations=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Evaluation Metrics with Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_improved(query):\n",
    "    boost = {\n",
    "          'abstract': 2.38,\n",
    "          'authors': 0.03,\n",
    "          'keywords': 0.52,\n",
    "          'organization_affiliated': 1.33,\n",
    "          'title': 0.20\n",
    "    }\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b74ce37af5f4adfa00842d193aeccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.99, 'mrr': 0.9655884920634924}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: minsearch_improved(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Evaluating RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Biomedical research question answering system.\n",
    "Your task is to analyze the relevance of the answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Questions Random Sample out of the Ground Truth Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00aade8a-7177-4b9e-8120-84770f278638</td>\n",
       "      <td>What are the challenges associated with using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01459f11-c3da-4d57-8e5e-0c9874459f54</td>\n",
       "      <td>What is the relationship between the geographi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0183843c-ce40-44be-a2c2-ae8b2110b8a4</td>\n",
       "      <td>What are the specific network metrics that are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0213f1ca-8b6c-462f-9688-d5cc0f3919ef</td>\n",
       "      <td>What is the significance of the finding that *...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>028c6a25-fcb6-446f-9d87-b8b718c0e4a2</td>\n",
       "      <td>Were there any other drugs besides ranitidine ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  00aade8a-7177-4b9e-8120-84770f278638   \n",
       "1  01459f11-c3da-4d57-8e5e-0c9874459f54   \n",
       "2  0183843c-ce40-44be-a2c2-ae8b2110b8a4   \n",
       "3  0213f1ca-8b6c-462f-9688-d5cc0f3919ef   \n",
       "4  028c6a25-fcb6-446f-9d87-b8b718c0e4a2   \n",
       "\n",
       "                                            question  \n",
       "0  What are the challenges associated with using ...  \n",
       "1  What is the relationship between the geographi...  \n",
       "2  What are the specific network metrics that are...  \n",
       "3  What is the significance of the finding that *...  \n",
       "4  Were there any other drugs besides ranitidine ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample one random question for each id\n",
    "sample_q_df = ground_truth_df.groupby('id').apply(lambda x: x.sample(n=1)).reset_index(drop=True)\n",
    "\n",
    "display(sample_q_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions data shape: (200, 2)\n",
      "Number of unique ids: 200\n"
     ]
    }
   ],
   "source": [
    "print('Questions data shape:', sample_q_df.shape)\n",
    "print('Number of unique ids:', len(sample_q_df['id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update RAG to use Boosted Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model='gemini-1.5-flash-001'):\n",
    "    search_results = minsearch_improved(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the challenges associated with using conventional battery technologies in epidermal electronic systems?\n",
      "Answer: The physical bulk, large mass, and high mechanical modulus of conventional battery technologies hinder efforts to achieve epidermal characteristics. Near-field power transfer schemes offer only a limited operating distance.  \n",
      "\n",
      "Evaluation: {'Relevance': 'RELEVANT', 'Explanation': 'The answer directly addresses the question by identifying key challenges of conventional batteries in epidermal electronics: bulkiness, weight, and stiffness. It also mentions limited operating distance of near-field power transfer, which is relevant to the topic of power sources in epidermal systems.'}\n"
     ]
    }
   ],
   "source": [
    "id0 = sample_q_df.loc[0, 'id']\n",
    "q0 = sample_q_df.loc[0, 'question']\n",
    "answer = rag(q0)\n",
    "prompt = prompt2_template.format(question=q0, answer_llm=answer)\n",
    "evaluation_llm = llm(prompt)\n",
    "json_string = evaluation_llm.strip().replace('json', '').replace('`', '')\n",
    "evaluation = json.loads(json_string)\n",
    "print(\"Question:\", q0)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Evaluation:\", evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Answers and Evaluations for the Sampled Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rate_limit_error():\n",
    "    print(\"Rate limit exceeded. Sleeping for 60 seconds...\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Gemini 1.5 Flash 001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_q = sample_q_df.to_dict(orient='records')\n",
    "evaluations = []\n",
    "for record in tqdm(sample_q):\n",
    "    question = record['question']\n",
    "    while True: # Retry loop\n",
    "        try:\n",
    "            answer = rag(question) \n",
    "            prompt = prompt2_template.format(\n",
    "                question=question,\n",
    "                answer_llm=answer\n",
    "            )\n",
    "            evaluation_llm = llm(prompt)\n",
    "            json_string = evaluation_llm.strip().replace('json', '').replace('`', '')\n",
    "            evaluation = json.loads(json_string)\n",
    "            break  # Exit the retry loop if successful\n",
    "        except Exception as e:\n",
    "            if \"Quota exceeded\" in str(e):\n",
    "                handle_rate_limit_error()\n",
    "            else:\n",
    "                # Handle other exceptions or re-raise them\n",
    "                raise e\n",
    "    evaluations.append((record, answer, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650e48e9ae7b4cb79d67a292744f5ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n"
     ]
    }
   ],
   "source": [
    "sample_q = sample_q_df[111:].to_dict(orient='records')\n",
    "for record in tqdm(sample_q):\n",
    "    question = record['question']\n",
    "    while True: # Retry loop\n",
    "        try:\n",
    "            answer = rag(question) \n",
    "            prompt = prompt2_template.format(\n",
    "                question=question,\n",
    "                answer_llm=answer\n",
    "            )\n",
    "            evaluation_llm = llm(prompt)\n",
    "            json_string = evaluation_llm.strip().replace('json', '').replace('`', '')\n",
    "            evaluation = json.loads(json_string)\n",
    "            break  # Exit the retry loop if successful\n",
    "        except Exception as e:\n",
    "            if \"Quota exceeded\" in str(e):\n",
    "                handle_rate_limit_error()\n",
    "            else:\n",
    "                # Handle other exceptions or re-raise them\n",
    "                raise e\n",
    "    evaluations.append((record, answer, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           132\n",
       "PARTLY_RELEVANT     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.66\n",
       "PARTLY_RELEVANT    0.34\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/rag-eval-gemini-1.5-flash-001.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Gemini 1.0 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f6e616fc5c4a26adf5ee1ecb4ff648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n",
      "Rate limit exceeded. Sleeping for 60 seconds...\n"
     ]
    }
   ],
   "source": [
    "sample_q = sample_q_df.to_dict(orient='records')\n",
    "evaluations = []\n",
    "for record in tqdm(sample_q):\n",
    "    question = record['question']\n",
    "    while True: # Retry loop\n",
    "        try:\n",
    "            answer = rag(question, model='gemini-1.0-pro') \n",
    "            prompt = prompt2_template.format(\n",
    "                question=question,\n",
    "                answer_llm=answer\n",
    "            )\n",
    "            evaluation_llm = llm(prompt)\n",
    "            json_string = evaluation_llm.strip().replace('json', '').replace('`', '')\n",
    "            evaluation = json.loads(json_string)\n",
    "            break  # Exit the retry loop if successful\n",
    "        except Exception as e:\n",
    "            if \"Quota exceeded\" in str(e):\n",
    "                handle_rate_limit_error()\n",
    "            else:\n",
    "                # Handle other exceptions or re-raise them\n",
    "                raise e\n",
    "    evaluations.append((record, answer, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           132\n",
       "PARTLY_RELEVANT     53\n",
       "NON_RELEVANT        15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.660\n",
       "PARTLY_RELEVANT    0.265\n",
       "NON_RELEVANT       0.075\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/rag-eval-gemini-1.0-pro.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
